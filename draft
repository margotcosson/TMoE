Article:
https://arxiv.org/abs/1701.07429

Code:
https://github.com/fchamroukhi/tMoE_m


Format:
Soutenance en distanciel: 7-8 minutes de présentation et 7-8 minutes de questions

Support:
Un power-point en anglais

Date:
04/01 à 13h45

Résumé de l'article (informel):
En gros quand on veut fitter des données qu'on pense issu de plusieurs distributions, on fait appel à un MoE (mixture of experts). Chaque donnée correspond
à la somme, pondérée par des probabilités d'appartenance (gating functions), de distributions de paramètres différents (dites experts). D'habitude on utilise
un mix de distributions gaussiennes mais ce n'est pas très robuste à la présence d'anomalies et de distributions avec des queues lourdes (heavy tails). 
Ainsi, l'auteur introduit un nouvel MoE basé sur les t distributions. Une t distribution c'est plus ou moins la généralisation robuste d'une gaussienne et 
c'est moins sensible à la présence d'anomalies et d'heavy tails. Pour entraîner ce TMoE, il utilise l'algorithme EM ou une version améliorée dite ECM dans
laquelle une étape E est intercalée entre deux sous-étapes M.

Plan:

Introduction - 45 sec
- Qu'est-ce qu'un MoE ? 
- Visualisation d'un cas qui motive la nécessité de cet outil
- Rappel bref du NMoE
- Application du NMoE sur ce cas => échec (visualisation) || Application sur un cas jouet
- Explication : présence d'anomalies et données issues de distribution à queue lourde
- Problématique: Quel MoE pour des données comportant des anomalies ou issues de distributions à queue lourde ?
- Annonce de plan

I/ Fondements théoriques - 2 min
A/ TMoE: définition, formule, Insister sur la différence entre MoE et simple mixture (basé sur 2 observations x et r)?
B/ EM Algorithm: rappels (mettre pseudo-code ?), présentation des formules, sketch de convergence, ECM

II/ Critique de l'article - 3 min
(A COMPLETER - Plan provisoire, sous-parties thématiques à venir quand ce sera plus clair)
A/ Points positifs
- Identifiability (Je crois qu'ils voulaient dire que chaque combinaison de paramètres trouvée donne une densité unique 
(ie deux combinaisons différentes de paramètres ne peuvent pas donner la même densité finale pour le MoE))
- Random initialization in the examples
- on peut utiliser AIC, BIC etc. pour déterminer le nb d'experts
B/ Points négatifs
- Laplace obtient à peu près les mêmes résultats (il est même meilleur dans certains cas)
- Les cas test sont incomplets:
    - (*) essayer cas test 2 avec > 5% d'outliers
    - (*) essayer cas test 2 avec des anomalies qui sont également des random variables (≠ déterministes)
    - (**) le cas real 2 (données climatiques) n'est pas discriminant entre les méthodes, essayer sur un cas plus compliqué
    - il ne dit pas quelle forme il prend pour mu(x_i, beta_k), s'il s'agit de la forme linéaire, pourquoi est-ce que c'est le choix adopté?
    - aucune précision sur l'utilisation de ECM vs EM dans les cas d'usage, isoler le bénéfice de la forme de la distribution t vs le bénéfice induit par l'ECM

III/ Exemple: application du TMoE - 2 min
(Je pense qu'il faut prendre un autre exemple que celui de l'article car on voit pas vraiment l'avantage concurrentiel du TMoE par rapport au NMoE, ce serait
peut-être pas mal de prendre un exemple de clustering aussi comme ça on étend le scope des exemples de l'article qui lui reste sur de la régression)
A/ Présentation des données et pourquoi elles pourraient être mieux fittées par TMoE que NMoE
B/ Résultats du TMoE
C/ Comparaison avec le NMoE

Conclusion - 30 sec
- Meilleur que NMoE sur les données avec des anomalies ou issues de distribution à queue lourde
- Possibilité de l'utiliser pour la density estimation, non-linear regression function approximation et le clustering (à détailler)
- autre ?

A FAIRE:
1. Créer ppt et faire l'intro
2. Partie I (fondements théoriques) sous-partie A
3. Partie I sous-partie B
4. Creuser les points négatifs (*) sur le cas test 2
5. Chercher un cas test discriminant sur les données climatiques (**)
6. Trouver quel algo (EM, ECM) est utilisé sur quelle forme (NMoE, LMoE, TMoE) dans les cas test ?

A CREUSER:
- C'est quoi les hierarchical representations dont il parle ? (voir page 7) et à la fin en ouverture il met "to extend the proposed models to 
the hierarchical MoE framework'.
- Page 11, je ne comprends pas en quoi IRLS est différent d'un algorithme de Newton classique ici ?
- Page 8, je ne comprends pas bien le choix de la distribution multinomiale pour Zi|ri et qu'est-ce que ça donnerait pour un autre choix ? Idem 
pour le choix de la fonction multinomiale logistique pour les gates probabilities.
- TMoE entrainé avec ECM ou EM dans exemple ? et NMoE et LMoE ? et comment isoler l'effet de l'ECM et de la nature intrinsèque de la distribution t ?
- Page 12, dérivation de la formule (31) à vérifier.
- Redémontrer les calculs faits dans l'EM à faire sur une feuille qu'on l'ait pour la soutenance.
- Page 20, sur l'absence de "confidence region" je n'ai pas compris de quoi il s'agissait.
